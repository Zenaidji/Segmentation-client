{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c04cab-3565-4710-bda9-b2fec8e9878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffeb6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYARROW_IGNORE_TIMEZONE'] = '1'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col,min,max,mean\n",
    "import time\n",
    "from pyspark.sql.functions import col, when, count, sum\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, TimestampType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, count, sum, min, max, mean,countDistinct, desc\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, datediff, lit\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import unix_timestamp, max as spark_max, sum as spark_sum, count as spark_count, col, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a5b620-d767-42d8-958e-97f633ef01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"category_id\", LongType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"user_session\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300afd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/18 21:03:42 WARN Utils: Your hostname, ghani-IdeaPad-3-14ADA05 resolves to a loopback address: 127.0.1.1; using 10.188.25.103 instead (on interface wlp2s0)\n",
      "24/06/18 21:03:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/18 21:03:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/18 21:03:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/06/18 21:03:43 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/06/18 21:03:43 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"clustering\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4d461c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Charger les fichiers CSV avec le schéma défini\n",
    "octobre_df = spark.read.csv(\"/home/ghani/Bureau/Data/2019-Oct.csv\", header=True, schema=schema)\n",
    "novembre_df = spark.read.csv(\"/home/ghani/Bureau/Data/2019-Nov.csv\", header=True, schema=schema)\n",
    "decembre_df = spark.read.csv(\"/home/ghani/Bureau/Data/2019-Dec.csv\", header=True, schema=schema)\n",
    "janvier_df = spark.read.csv(\"/home/ghani/Bureau/Data/2020-Jan.csv\", header=True, schema=schema)\n",
    "fevrier_df = spark.read.csv(\"/home/ghani/Bureau/Data/2020-Feb.csv\", header=True, schema=schema)\n",
    "mars_df = spark.read.csv(\"/home/ghani/Bureau/Data/2020-Mar.csv\", header=True, schema=schema)\n",
    "avril_df = spark.read.csv(\"/home/ghani/Bureau/Data/2020-Apr.csv\", header=True, schema=schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa346990",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = (octobre_df\n",
    "               .union(novembre_df)\n",
    "               .union(decembre_df)\n",
    "               .union(janvier_df)\n",
    "               .union(fevrier_df)\n",
    "               .union(mars_df)\n",
    "               .union(avril_df))\n",
    "\n",
    "# Prendre un échantillon de 10 % du DataFrame combiné\n",
    "#combined_sample = combined_df.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ef100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_sample.show()\n",
    "#combined_sample.cache()\n",
    "#combined_sample.count()\n",
    "#combined_df.cache()\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Calculer le nombre d'éléments distincts dans la colonne 'user_id'\n",
    "distinct_user_count = combined_df.select(countDistinct('user_id')).collect()[0][0]\n",
    "\n",
    "print(f\"Nombre d'utilisateurs distincts : {distinct_user_count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = combined_df.withColumn(\"event_time\", unix_timestamp(col(\"event_time\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n",
    "\n",
    "# Filtrer les événements d'achat\n",
    "data_purchase = data.filter(col(\"event_type\") == \"purchase\")\n",
    "\n",
    "# Dernière date dans le jeu de données\n",
    "last_date = data.agg(spark_max(\"event_time\")).collect()[0][0] + datetime.timedelta(days=1)\n",
    "\n",
    "# Fonction UDF pour calculer la récence\n",
    "recency_udf = udf(lambda x: (last_date - x).days, IntegerType())\n",
    "\n",
    "# Calculer RFM\n",
    "rfm = data_purchase.groupBy(\"user_id\").agg(\n",
    "    recency_udf(spark_max(\"event_time\")).alias(\"Recency\"),\n",
    "    spark_count(\"user_session\").alias(\"Frequency\"),\n",
    "    spark_sum(\"price\").alias(\"Monetary\")\n",
    ")\n",
    "\n",
    "# Afficher les premières lignes des données RFM\n",
    "rfm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b2810f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfm2=spark.read.parquet('/home/ghani/Bureau/Data/rfm.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d9d8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2064899"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfm2.cache()\n",
    "rfm2.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d9e37c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+------------------+\n",
      "|  user_id|Recency|Frequency|          Monetary|\n",
      "+---------+-------+---------+------------------+\n",
      "|513724405|    189|        3|            272.86|\n",
      "|543838936|    136|        2|            810.81|\n",
      "|534285038|    171|        2|             79.75|\n",
      "|517864279|    213|        1|            196.68|\n",
      "|541774796|    213|        1|             29.51|\n",
      "|555485382|    213|        1|            957.53|\n",
      "|539702146|    117|        3|2458.2400000000002|\n",
      "|512386086|    134|      806|473119.38999999984|\n",
      "|545616788|     70|       88|          33947.31|\n",
      "|513509085|     75|      155|180843.02999999997|\n",
      "|513161483|    175|        6|           6235.88|\n",
      "|513812036|     80|        6|            307.19|\n",
      "|515494405|      2|       50|47981.880000000005|\n",
      "|539319122|     77|        6|           1289.26|\n",
      "|550397911|     17|        2|            109.14|\n",
      "|519311105|    213|        1|            111.79|\n",
      "|513723185|     54|        7|           1281.94|\n",
      "|536615687|    169|       44| 7705.519999999999|\n",
      "|518427651|     34|        3|            254.32|\n",
      "|555522019|    213|        1|             130.7|\n",
      "+---------+-------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfm2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370a76bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|          Monetary|\n",
      "+-------+------------------+\n",
      "|  count|           2064899|\n",
      "|   mean|  996.391217129683|\n",
      "| stddev|3678.6571150987147|\n",
      "|    min|              0.31|\n",
      "|    max| 790120.9400000002|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfm2.describe(\"Monetary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7e4546",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+------------------+\n",
      "|  user_id|Recency|Frequency|          Monetary|\n",
      "+---------+-------+---------+------------------+\n",
      "|553431815|      1|      888| 790120.9400000002|\n",
      "|549109608|     25|     1383| 750149.9700000001|\n",
      "|569333570|     18|      488| 689273.0899999996|\n",
      "|513230794|     40|     1373| 647798.6500000001|\n",
      "|595104624|     36|      458|         605792.74|\n",
      "|513320236|      3|     1159| 567148.1000000001|\n",
      "|610871788|      1|      407| 561013.4799999989|\n",
      "|568782581|     53|     2120| 540447.6100000002|\n",
      "|598224683|      1|     1059| 539601.6799999999|\n",
      "|538216048|     51|      417|523403.12999999995|\n",
      "|563599039|     78|      958|474648.45999999996|\n",
      "|512386086|    134|      806|473119.38999999984|\n",
      "|515428951|     52|      490|447804.55000000005|\n",
      "|516010934|     35|      482|         403633.97|\n",
      "|512409624|     38|      521| 386011.6199999999|\n",
      "|518514099|     21|      410| 368759.5599999999|\n",
      "|562104312|     19|      260| 351876.0299999998|\n",
      "|514726585|     34|      341|336402.48000000004|\n",
      "|515384420|      7|      218|332926.29000000004|\n",
      "|534545940|    108|      315| 331185.9799999999|\n",
      "+---------+-------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:============================================>              (6 + 2) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sorted_users = rfm2.orderBy(desc('Monetary'))\n",
    "sorted_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c5268ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm2 = rfm2.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a06f5747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2064899"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfm2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a4d4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_monetary_below_threshold(df, lower_bound, upper_bound):\n",
    "    \n",
    "    filtered_df = df.filter((col(\"Monetary\") >= lower_bound) & (col(\"Monetary\") <= upper_bound))\n",
    "    \n",
    "    return filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef587e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Average_Monetary|\n",
      "+----------------+\n",
      "|996.391217129683|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_monetary = rfm2.agg(avg('Monetary').alias('Average_Monetary'))\n",
    "\n",
    "\n",
    "average_monetary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f87d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_columns = rfm2.columns[1:] \n",
    "\n",
    "# Assembler les colonnes de features en un seul vecteur de features\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(rfm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37bccac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9164002514982978\n",
      "Cluster Centers: \n",
      "[ 91.32736519   2.20662947 463.80008394]\n",
      "[3.87777778e+01 4.80583333e+02 3.32712071e+05]\n",
      "[5.26140351e+01 1.63654971e+02 1.01027941e+05]\n",
      "[  73.85195734   10.00541381 4008.15981987]\n",
      "[   53.10647596    74.38895511 38145.15316647]\n",
      "[   62.92846229    30.16307821 13710.96435963]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Séparer les données en 60% pour l'entraînement et 40% pour le test\n",
    "training_data, test_data = data.randomSplit([0.6, 0.4], seed=1234)\n",
    "\n",
    "# Entraîner le modèle K-means uniquement sur les données d'entraînement\n",
    "kmeans = KMeans().setK(6).setSeed(1)  # Spécifier le nombre de clusters (k) et la graine (seed) pour la reproductibilité\n",
    "model = kmeans.fit(training_data)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Évaluer la performance du clustering en utilisant l'évaluateur de clustering\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Afficher les résultats du clustering\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "# Arrêter la session Spark\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab47d467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+------------------+--------------------+----------+\n",
      "|  user_id|Recency|Frequency|          Monetary|            features|prediction|\n",
      "+---------+-------+---------+------------------+--------------------+----------+\n",
      "| 96369466|     12|        1|             50.71|    [12.0,1.0,50.71]|         0|\n",
      "|128968633|     55|        4|            714.01|   [55.0,4.0,714.01]|         0|\n",
      "|238417100|     80|        1|            116.32|   [80.0,1.0,116.32]|         0|\n",
      "|286168756|     51|        1|             28.29|    [51.0,1.0,28.29]|         0|\n",
      "|307478059|     92|        1|             33.46|    [92.0,1.0,33.46]|         0|\n",
      "|310829160|     71|        1|            105.54|   [71.0,1.0,105.54]|         0|\n",
      "|338689827|    130|        2|1110.3500000000001|[130.0,2.0,1110.3...|         0|\n",
      "|355749922|     21|        1|            106.57|   [21.0,1.0,106.57]|         0|\n",
      "|366237542|    155|        2|300.15999999999997|[155.0,2.0,300.15...|         0|\n",
      "|369288911|     48|        1|            203.58|   [48.0,1.0,203.58]|         0|\n",
      "|373117927|     24|        1|             77.22|    [24.0,1.0,77.22]|         0|\n",
      "|375847278|     39|        1|             51.48|    [39.0,1.0,51.48]|         0|\n",
      "|386929437|    143|        1|              36.5|    [143.0,1.0,36.5]|         0|\n",
      "|389051600|    172|        1|            229.91|  [172.0,1.0,229.91]|         0|\n",
      "|393237889|     70|        3|1349.9099999999999|[70.0,3.0,1349.90...|         0|\n",
      "|395578780|     70|        1|            108.09|   [70.0,1.0,108.09]|         0|\n",
      "|400894696|     37|        1|            360.34|   [37.0,1.0,360.34]|         0|\n",
      "|408200609|    158|        1|            216.61|  [158.0,1.0,216.61]|         0|\n",
      "|415259918|     55|        1|             72.05|    [55.0,1.0,72.05]|         0|\n",
      "|422223634|     98|        1|            941.81|   [98.0,1.0,941.81]|         0|\n",
      "+---------+-------+---------+------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806baa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_columns = rfm2.columns[1:]  # Supposant que la première colonne est l'ID\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(rfm2)\n",
    "\n",
    "# Conversion en Pandas DataFrame pour scikit-learn\n",
    "pandas_df = data.select(\"features\").toPandas()\n",
    "\n",
    "# Standardisation des données (optionnel mais souvent recommandé pour DBSCAN)\n",
    "scaler = StandardScaler()\n",
    "pandas_df['scaled_features'] = scaler.fit_transform(pandas_df['features'].apply(lambda x: np.array(x.toArray())).tolist())\n",
    "\n",
    "# Entraînement de DBSCAN avec scikit-learn\n",
    "eps = 0.3  # Rayon de voisinage\n",
    "min_samples = 5  # Nombre minimum de points pour former un cluster\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "pandas_df['cluster'] = dbscan.fit_predict(pandas_df['scaled_features'])\n",
    "\n",
    "# Évaluation du clustering (optionnel)\n",
    "silhouette_score = metrics.silhouette_score(pandas_df['scaled_features'], pandas_df['cluster'])\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "\n",
    "# Convertir les prédictions en DataFrame Spark pour une utilisation ultérieure\n",
    "predictions_spark = spark.createDataFrame(pd.DataFrame(pandas_df))\n",
    "\n",
    "# Affichage des résultats du clustering dans Spark DataFrame\n",
    "predictions_spark.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.write.parquet('/home/ghani/Bureau/Data/rfm.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_sample=spark.read.parquet('/home/ghani/Bureau/Data/combined_sample.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a2c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#counts = combined_sample.select([count(col(c)).alias(c) for c in combined_sample.columns])\n",
    "#counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#category_code_distinc = combined_sample.select([countDistinct(col(c)).alias(c) for c in combined_sample.columns])\n",
    "\n",
    "#category_code_distinc.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d77082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#counts_null = combined_sample.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in combined_sample.columns])\n",
    "#counts_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a67404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# counts_null_brand_category = combined_sample.select(\n",
    "#     sum(when((col(\"category_code\").isNull()) & (col(\"brand\").isNull()), 1).otherwise(0)).alias(\"null_count\")\n",
    "# )\n",
    "# counts_null_brand_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#event_type_counts = combined_sample.groupBy(\"event_type\").agg(count(\"*\").alias(\"nombre\"))\n",
    "#event_type_counts=event_type_counts.toPandas()\n",
    "#event_type_counts.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_sample.describe(['price']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#purchase_df=combined_sample.filter(col(\"event_type\")==\"purchase\")\n",
    "#purchase_df.describe([\"price\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3952aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_brands = combined_sample.groupBy(\"brand\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "# top_brands_df = top_brands.toPandas()\n",
    "\n",
    "\n",
    "# top_categories = combined_sample.groupBy(\"category_code\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "# top_categories_df = top_categories.toPandas()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(14, 7))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.pie(top_brands_df['count'], labels=top_brands_df['brand'], autopct='%1.1f%%', startangle=140)\n",
    "# plt.title('Top 10 Brands')\n",
    "\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.pie(top_categories_df['count'], labels=top_categories_df['category_code'], autopct='%1.1f%%', startangle=140)\n",
    "# plt.title('Top 10 Categories')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ea903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# total = event_type_counts['nombre'].sum()\n",
    "# event_type_counts['percentage'] = (event_type_counts['nombre'] / total) * 100\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# bars = plt.bar(event_type_counts['event_type'], event_type_counts['nombre'], color=['green', 'blue', 'red'])\n",
    "\n",
    "\n",
    "# for bar, percentage in zip(bars, event_type_counts['percentage']):\n",
    "#     yval = bar.get_height()\n",
    "#     plt.text(bar.get_x() + bar.get_width()/2, yval, f'{percentage:.1f}%', va='bottom')  \n",
    "\n",
    "# plt.xlabel('Event Type')\n",
    "# plt.ylabel('Nombre')\n",
    "# plt.title('Histogram of Event Types with Percentages')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combined_sample = combined_sample.filter(~(col(\"category_code\").isNull() & col(\"brand\").isNull()))\n",
    "\n",
    "# combined_sample.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ae211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_null_brand_category = combined_sample.select(\n",
    "#     sum(when((col(\"category_code\").isNull()) & (col(\"brand\").isNull()), 1).otherwise(0)).alias(\"null_count\")\n",
    "# )\n",
    "# counts_null_brand_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ba7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before=combined_sample.count()\n",
    "# combined_sample = combined_sample.dropDuplicates()\n",
    "# after=combined_sample.count()\n",
    "# print(before-after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_sample = combined_sample.filter((col(\"user_session\").isNotNull()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_null2 = combined_sample.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in combined_sample.columns])\n",
    "# counts_null2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_sample.write.parquet('/home/ghani/Bureau/Data/combined_sample.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_sample = combined_sample.fillna({'category_code': 'unknown_category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2706e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_df[\"cluster\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e697c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
