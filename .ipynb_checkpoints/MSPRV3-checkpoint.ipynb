{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c0cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYARROW_IGNORE_TIMEZONE'] = '1'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col,min,max,mean\n",
    "import time\n",
    "from pyspark.sql.functions import col, when, count,sum,split\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, TimestampType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, count, sum, min, max, mean,countDistinct\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, datediff, lit\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import unix_timestamp, max as spark_max, sum as spark_sum, count as spark_count, col, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import datetime\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27542af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"category_id\", LongType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"user_session\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139b8f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 19:54:43 WARN Utils: Your hostname, ghani-IdeaPad-3-14ADA05 resolves to a loopback address: 127.0.1.1; using 10.188.25.103 instead (on interface wlp2s0)\n",
      "24/06/20 19:54:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/20 19:54:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/20 19:54:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"msprv3\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be8d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "octobre_df = spark.read.csv(\"/home/ghani/Bureau/Data/2019-Oct.csv\", header=True, schema=schema)\n",
    "novembre_df = spark.read.csv(\"/home/ghani/Bureau/Data/2019-Nov.csv\", header=True, schema=schema)\n",
    "decembre_df = spark.read.csv(\"/home/ghani/Bureau/Data/2019-Dec.csv\", header=True, schema=schema)\n",
    "janvier_df = spark.read.csv(\"/home/ghani/Bureau/Data/2020-Jan.csv\", header=True, schema=schema)\n",
    "fevrier_df = spark.read.csv(\"/home/ghani/Bureau/Data/2020-Feb.csv\", header=True, schema=schema)\n",
    "mars_df = spark.read.csv(\"/home/ghani/Bureau/Data/2020-Mar.csv\", header=True, schema=schema)\n",
    "avril_df = spark.read.csv(\"/home/ghani/Bureau/Data/2020-Apr.csv\", header=True, schema=schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde010cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = (octobre_df\n",
    "               .union(novembre_df)\n",
    "               .union(decembre_df)\n",
    "               .union(janvier_df)\n",
    "               .union(fevrier_df)\n",
    "               .union(mars_df)\n",
    "               .union(avril_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03175aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop('brand', 'category_id', 'product_id', 'user_session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, split, when\n",
    "\n",
    "def extract_primary_category(df: DataFrame, column_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cette fonction prend un DataFrame et le nom d'une colonne contenant des codes catégoriels séparés par des points.\n",
    "    Elle crée une nouvelle colonne 'primary_category' qui contient la première catégorie de la colonne spécifiée.\n",
    "    Si la valeur originale est null, 'unknown' sera utilisé à la place.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame Spark à transformer.\n",
    "    column_name (str): Le nom de la colonne à traiter.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Un nouveau DataFrame avec la colonne 'primary_category' ajoutée.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "        \"primary_category\",\n",
    "        when(col(column_name).isNull(), \"unknown\")\n",
    "        .otherwise(split(col(column_name), \"\\.\").getItem(0))\n",
    "    )\n",
    "\n",
    "# Utilisation de la fonction\n",
    "spark = SparkSession.builder.appName(\"Extract Primary Category\").getOrCreate()\n",
    "\n",
    "# Supposons que combined_df est votre DataFrame Spark initial\n",
    "# Vous devez remplacer 'category_code' par le nom réel de votre colonne si différent\n",
    "new_df = extract_primary_category(combined_df, \"category_code\")\n",
    "\n",
    "# Afficher le résultat pour vérifier\n",
    "new_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
